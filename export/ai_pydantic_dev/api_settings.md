---
url: https://ai.pydantic.dev/api/settings/
session_id: pydantic_ai_docs
updated_dt: 2025-03-14T09:20:41.798948
---
[ Skip to content ](https://ai.pydantic.dev/api/settings/#pydantic_aisettings)
[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/ "PydanticAI")
PydanticAI 
pydantic_ai.settings 
Type to start searching
[ pydantic/pydantic-ai  ](https://github.com/pydantic/pydantic-ai "Go to repository")
[ ![logo](https://ai.pydantic.dev/img/logo-white.svg) ](https://ai.pydantic.dev/ "PydanticAI") PydanticAI 
[ pydantic/pydantic-ai  ](https://github.com/pydantic/pydantic-ai "Go to repository")
  * [ Introduction  ](https://ai.pydantic.dev/)
  * [ Installation  ](https://ai.pydantic.dev/install/)
  * [ Getting Help  ](https://ai.pydantic.dev/help/)
  * [ Contributing  ](https://ai.pydantic.dev/contributing/)
  * [ Troubleshooting  ](https://ai.pydantic.dev/troubleshooting/)
  * Documentation  Documentation 
    * [ Agents  ](https://ai.pydantic.dev/agents/)
    * [ Models  ](https://ai.pydantic.dev/models/)
    * [ Dependencies  ](https://ai.pydantic.dev/dependencies/)
    * [ Function Tools  ](https://ai.pydantic.dev/tools/)
    * [ Common Tools  ](https://ai.pydantic.dev/common_tools/)
    * [ Results  ](https://ai.pydantic.dev/results/)
    * [ Messages and chat history  ](https://ai.pydantic.dev/message-history/)
    * [ Testing and Evals  ](https://ai.pydantic.dev/testing-evals/)
    * [ Debugging and Monitoring  ](https://ai.pydantic.dev/logfire/)
    * [ Multi-agent Applications  ](https://ai.pydantic.dev/multi-agent-applications/)
    * [ Graphs  ](https://ai.pydantic.dev/graph/)
    * [ Image, Audio & Document Input  ](https://ai.pydantic.dev/input/)
    * [ Command Line Interface (CLI)  ](https://ai.pydantic.dev/cli/)
  * [ Examples  ](https://ai.pydantic.dev/examples/)
Examples 
    * [ Pydantic Model  ](https://ai.pydantic.dev/examples/pydantic-model/)
    * [ Weather agent  ](https://ai.pydantic.dev/examples/weather-agent/)
    * [ Bank support  ](https://ai.pydantic.dev/examples/bank-support/)
    * [ SQL Generation  ](https://ai.pydantic.dev/examples/sql-gen/)
    * [ Flight booking  ](https://ai.pydantic.dev/examples/flight-booking/)
    * [ RAG  ](https://ai.pydantic.dev/examples/rag/)
    * [ Stream markdown  ](https://ai.pydantic.dev/examples/stream-markdown/)
    * [ Stream whales  ](https://ai.pydantic.dev/examples/stream-whales/)
    * [ Chat App with FastAPI  ](https://ai.pydantic.dev/examples/chat-app/)
    * [ Question Graph  ](https://ai.pydantic.dev/examples/question-graph/)
  * API Reference  API Reference 
    * [ pydantic_ai.agent  ](https://ai.pydantic.dev/api/agent/)
    * [ pydantic_ai.tools  ](https://ai.pydantic.dev/api/tools/)
    * [ pydantic_ai.common_tools  ](https://ai.pydantic.dev/api/common_tools/)
    * [ pydantic_ai.result  ](https://ai.pydantic.dev/api/result/)
    * [ pydantic_ai.messages  ](https://ai.pydantic.dev/api/messages/)
    * [ pydantic_ai.exceptions  ](https://ai.pydantic.dev/api/exceptions/)
    * pydantic_ai.settings  [ pydantic_ai.settings  ](https://ai.pydantic.dev/api/settings/) Table of contents 
      * [ settings  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings)
      * [ ModelSettings  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings)
        * [ max_tokens  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.max_tokens)
        * [ temperature  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.temperature)
        * [ top_p  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.top_p)
        * [ timeout  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.timeout)
        * [ parallel_tool_calls  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.parallel_tool_calls)
        * [ seed  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.seed)
        * [ presence_penalty  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.presence_penalty)
        * [ frequency_penalty  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.frequency_penalty)
        * [ logit_bias  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.logit_bias)
    * [ pydantic_ai.usage  ](https://ai.pydantic.dev/api/usage/)
    * [ pydantic_ai.format_as_xml  ](https://ai.pydantic.dev/api/format_as_xml/)
    * [ pydantic_ai.models  ](https://ai.pydantic.dev/api/models/base/)
    * [ pydantic_ai.models.openai  ](https://ai.pydantic.dev/api/models/openai/)
    * [ pydantic_ai.models.anthropic  ](https://ai.pydantic.dev/api/models/anthropic/)
    * [ pydantic_ai.models.bedrock  ](https://ai.pydantic.dev/api/models/bedrock/)
    * [ pydantic_ai.models.cohere  ](https://ai.pydantic.dev/api/models/cohere/)
    * [ pydantic_ai.models.gemini  ](https://ai.pydantic.dev/api/models/gemini/)
    * [ pydantic_ai.models.vertexai  ](https://ai.pydantic.dev/api/models/vertexai/)
    * [ pydantic_ai.models.groq  ](https://ai.pydantic.dev/api/models/groq/)
    * [ pydantic_ai.models.mistral  ](https://ai.pydantic.dev/api/models/mistral/)
    * [ pydantic_ai.models.test  ](https://ai.pydantic.dev/api/models/test/)
    * [ pydantic_ai.models.function  ](https://ai.pydantic.dev/api/models/function/)
    * [ pydantic_ai.models.fallback  ](https://ai.pydantic.dev/api/models/fallback/)
    * [ pydantic_ai.providers  ](https://ai.pydantic.dev/api/providers/)
    * [ pydantic_graph  ](https://ai.pydantic.dev/api/pydantic_graph/graph/)
    * [ pydantic_graph.nodes  ](https://ai.pydantic.dev/api/pydantic_graph/nodes/)
    * [ pydantic_graph.state  ](https://ai.pydantic.dev/api/pydantic_graph/state/)
    * [ pydantic_graph.mermaid  ](https://ai.pydantic.dev/api/pydantic_graph/mermaid/)
    * [ pydantic_graph.exceptions  ](https://ai.pydantic.dev/api/pydantic_graph/exceptions/)


Table of contents 
  * [ settings  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings)
  * [ ModelSettings  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings)
    * [ max_tokens  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.max_tokens)
    * [ temperature  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.temperature)
    * [ top_p  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.top_p)
    * [ timeout  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.timeout)
    * [ parallel_tool_calls  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.parallel_tool_calls)
    * [ seed  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.seed)
    * [ presence_penalty  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.presence_penalty)
    * [ frequency_penalty  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.frequency_penalty)
    * [ logit_bias  ](https://ai.pydantic.dev/api/settings/#pydantic_ai.settings.ModelSettings.logit_bias)


# `pydantic_ai.settings`
###  ModelSettings
Bases: `TypedDict[](https://typing-extensions.readthedocs.io/en/latest/index.html#typing_extensions.TypedDict "typing_extensions.TypedDict")`
Settings to configure an LLM.
Here we include only settings which apply to multiple models / model providers, though not all of these settings are supported by all models.
Source code in `pydantic_ai_slim/pydantic_ai/settings.py`
```
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
```
| ```
classModelSettings(TypedDict, total=False):
"""Settings to configure an LLM.
  Here we include only settings which apply to multiple models / model providers,
  though not all of these settings are supported by all models.
  """
  max_tokens: int
"""The maximum number of tokens to generate before stopping.
  Supported by:
  * Gemini
  * Anthropic
  * OpenAI
  * Groq
  * Cohere
  * Mistral
  * Bedrock
  """
  temperature: float
"""Amount of randomness injected into the response.
  Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's
  maximum `temperature` for creative and generative tasks.
  Note that even with `temperature` of `0.0`, the results will not be fully deterministic.
  Supported by:
  * Gemini
  * Anthropic
  * OpenAI
  * Groq
  * Cohere
  * Mistral
  * Bedrock
  """
  top_p: float
"""An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
  So 0.1 means only the tokens comprising the top 10% probability mass are considered.
  You should either alter `temperature` or `top_p`, but not both.
  Supported by:
  * Gemini
  * Anthropic
  * OpenAI
  * Groq
  * Cohere
  * Mistral
  * Bedrock
  """
  timeout: float | Timeout
"""Override the client-level default timeout for a request, in seconds.
  Supported by:
  * Gemini
  * Anthropic
  * OpenAI
  * Groq
  * Mistral
  """
  parallel_tool_calls: bool
"""Whether to allow parallel tool calls.
  Supported by:
  * OpenAI (some models, not o1)
  * Groq
  * Anthropic
  """
  seed: int
"""The random seed to use for the model, theoretically allowing for deterministic results.
  Supported by:
  * OpenAI
  * Groq
  * Cohere
  * Mistral
  """
  presence_penalty: float
"""Penalize new tokens based on whether they have appeared in the text so far.
  Supported by:
  * OpenAI
  * Groq
  * Cohere
  * Gemini
  * Mistral
  """
  frequency_penalty: float
"""Penalize new tokens based on their existing frequency in the text so far.
  Supported by:
  * OpenAI
  * Groq
  * Cohere
  * Gemini
  * Mistral
  """
  logit_bias: dict[str, int]
"""Modify the likelihood of specified tokens appearing in the completion.
  Supported by:
  * OpenAI
  * Groq
  """

```
  
---|---  
####  max_tokens `instance-attribute`
```
max_tokens: int[](https://docs.python.org/3/library/functions.html#int)

```

The maximum number of tokens to generate before stopping.
Supported by:
  * Gemini
  * Anthropic
  * OpenAI
  * Groq
  * Cohere
  * Mistral
  * Bedrock


####  temperature `instance-attribute`
```
temperature: float[](https://docs.python.org/3/library/functions.html#float)

```

Amount of randomness injected into the response.
Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's maximum `temperature` for creative and generative tasks.
Note that even with `temperature` of `0.0`, the results will not be fully deterministic.
Supported by:
  * Gemini
  * Anthropic
  * OpenAI
  * Groq
  * Cohere
  * Mistral
  * Bedrock


####  top_p `instance-attribute`
```
top_p: float[](https://docs.python.org/3/library/functions.html#float)

```

An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
So 0.1 means only the tokens comprising the top 10% probability mass are considered.
You should either alter `temperature` or `top_p`, but not both.
Supported by:
  * Gemini
  * Anthropic
  * OpenAI
  * Groq
  * Cohere
  * Mistral
  * Bedrock


####  timeout `instance-attribute`
```
timeout: float[](https://docs.python.org/3/library/functions.html#float) | Timeout

```

Override the client-level default timeout for a request, in seconds.
Supported by:
  * Gemini
  * Anthropic
  * OpenAI
  * Groq
  * Mistral


####  parallel_tool_calls `instance-attribute`
```
parallel_tool_calls: bool[](https://docs.python.org/3/library/functions.html#bool)

```

Whether to allow parallel tool calls.
Supported by:
  * OpenAI (some models, not o1)
  * Groq
  * Anthropic


####  seed `instance-attribute`
```
seed: int[](https://docs.python.org/3/library/functions.html#int)

```

The random seed to use for the model, theoretically allowing for deterministic results.
Supported by:
  * OpenAI
  * Groq
  * Cohere
  * Mistral


####  presence_penalty `instance-attribute`
```
presence_penalty: float[](https://docs.python.org/3/library/functions.html#float)

```

Penalize new tokens based on whether they have appeared in the text so far.
Supported by:
  * OpenAI
  * Groq
  * Cohere
  * Gemini
  * Mistral


####  frequency_penalty `instance-attribute`
```
frequency_penalty: float[](https://docs.python.org/3/library/functions.html#float)

```

Penalize new tokens based on their existing frequency in the text so far.
Supported by:
  * OpenAI
  * Groq
  * Cohere
  * Gemini
  * Mistral


####  logit_bias `instance-attribute`
```
logit_bias: dict[](https://docs.python.org/3/library/stdtypes.html#dict)[str[](https://docs.python.org/3/library/stdtypes.html#str), int[](https://docs.python.org/3/library/functions.html#int)]

```

Modify the likelihood of specified tokens appearing in the completion.
Supported by:
  * OpenAI
  * Groq


© Pydantic Services Inc. 2024 to present 
