"""Default description (change me)"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/routes/crawler.ipynb.

# %% auto 0
__all__ = ['default_browser_config', 'Crawler_Route_NotSuccess', 'scrape_url', 'crawl_urls']

# %% ../../nbs/routes/crawler.ipynb 2
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy

from ..client.ResponseGetData import ResponseGetDataCrawler

# %% ../../nbs/routes/crawler.ipynb 3
import os
from typing import Callable
from agent_mafia.client import MafiaError as amme
from agent_mafia.utils import convert as amcv

# %% ../../nbs/routes/crawler.ipynb 5
default_browser_config = BrowserConfig(
    browser_type="chromium",
    headless=True,
    verbose=True,
    extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
)

# %% ../../nbs/routes/crawler.ipynb 6
class Crawler_Route_NotSuccess(amme.MafiaError):
    def __init__(self, message = None, exception = None):
        super().__init__(message = message, exception = exception)

# %% ../../nbs/routes/crawler.ipynb 7
async def scrape_url(
    url: str,
    session_id: str,
    browser_config: BrowserConfig = None,
    crawler_config: CrawlerRunConfig = None,
    storage_fn: Callable = None,
):

    browser_config = browser_config or default_browser_config

    res = None
    content = None

    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            crawler_config = crawler_config or CrawlerRunConfig(
                cache_mode=CacheMode.ENABLED,
                
            )

            res = await crawler.arun(
                url=url,
                config=crawler_config,
                session_id=session_id,
                timeout=15,
            )

            print(res)

    except NotImplementedError as e:
        raise Crawler_Route_NotSuccess(
            message="have you run create4ai-create and create4ai-doctor? in terminal",
            exception=e,
        )
    
    except Exception as e:
        raise Crawler_Route_NotSuccess(
            exception=e,
        ) from e

    if not res.success:
        raise Crawler_Route_NotSuccess(
            message=f"error crawling {url} - {res.error_message}"
        )

    rgd = ResponseGetDataCrawler.from_res(res)

    if storage_fn:
        storage_fn(
            data={
                "content": res.markdown,
                "source": session_id,
                "url": res.url,
            }
        )

    return rgd

# %% ../../nbs/routes/crawler.ipynb 9
async def crawl_urls(
    starting_url: str,
    session_id: str,
    output_path: str,
    crawler_config: CrawlerRunConfig = None,
    browser_config: BrowserConfig = None,
    storage_fn: Callable = None,
):
    browser_config = browser_config or default_browser_config
    try:

        results = []
        async with AsyncWebCrawler(config=browser_config) as crawler:
            async for res in await crawler.arun(
                starting_url,
                config=crawler_config,
                timeout=15,
                session_id=session_id,
            ):
                rgd = ResponseGetDataCrawler.from_res(res)

                if storage_fn:
                    storage_fn(
                        output_path=f"{os.path.join(
                            output_path, amcv.convert_url_file_name(res.url))}.md",
                        data={
                            "content": res.markdown,
                            "source": session_id,
                            "url": res.url,
                        },
                    )

                results.append(rgd)

        return results

    except NotImplementedError as e:
        raise Crawler_Route_NotSuccess(
            message="have you run create4ai-create and create4ai-doctor? in terminal",
            exception=e,
        )

    except Exception as e:
        raise Crawler_Route_NotSuccess(
            exception=e,
        ) from e
