{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# default_exp routes.openai"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Default Title (change me)\n", "> Default description (change me)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "from openai import AsyncOpenAI\n", "from typing import Union, Dict, List\n", "\n", "import json\n", "from dotenv import load_dotenv\n", "import os\n", "\n", "load_dotenv()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "default_model = os.environ.get(\"LLM_MODEL\", \"gpt-4o-mini-2024-07-18\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "default_async_openai_client: AsyncOpenAI = AsyncOpenAI(\n", "    api_key=os.getenv(\"OPENAI_API_KEY\")\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "async def generate_openai_chat(\n", "    messages,\n", "    async_client: AsyncOpenAI = None,\n", "    model: str = None,\n", "    response_format: Union[Dict[str, str], None] = None,\n", "    return_raw: bool = False,\n", "    debug_prn: bool = False,\n", "):\n", "    model = model or default_model\n", "    if debug_prn:\n", "        print(\"\ud83d\udcda - querying LLM\")\n", "\n", "    async_client = async_client or default_async_openai_client\n", "\n", "    res = await async_client.chat.completions.create(\n", "        model=model, messages=messages, response_format=response_format\n", "    )\n", "\n", "    if return_raw:\n", "        return res\n", "\n", "    res = res.choices[0].message.content\n", "\n", "    if response_format.get(\"type\") == \"json_object\":\n", "        res = json.loads(res)\n", "\n", "    return res"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "async def generate_openai_embbedding(\n", "    text,\n", "    async_client: AsyncOpenAI = None,\n", "    model=\"text-embedding-3-small\",\n", "    return_raw: bool = False,\n", "    debug_prn: bool = False,\n", ") -> List[float]:\n", "\n", "    if debug_prn:\n", "        print(\"\ud83d\udcda - starting LLM\")\n", "\n", "    async_client = async_client or default_async_openai_client\n", "\n", "    res = await async_client.embeddings.create(model=model, input=text)\n", "\n", "    if return_raw:\n", "        return res\n", "\n", "    return res.data[0].embedding"]}], "metadata": {"jupytext": {"split_at_heading": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 4}