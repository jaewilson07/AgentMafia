{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# | default_exp implementations.scrape_urls"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Default Title (change me)\n",
                "\n",
                "> Default description (change me)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# | export\n",
                "import os\n",
                "from typing import List\n",
                "\n",
                "from functools import partial\n",
                "\n",
                "import agent_mafia.utils.files as amfi\n",
                "import agent_mafia.utils.convert as amcv\n",
                "import agent_mafia.utils.chunking as amcn\n",
                "import agent_mafia.utils.chunk_execution as amce\n",
                "\n",
                "from agent_mafia.routes import storage as storage_routes\n",
                "from agent_mafia.routes import crawler as crawler_routes\n",
                "from agent_mafia.classes import Crawler_ProcessedChunk as pc\n",
                "import agent_mafia.client.MafiaError as amme\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| hide\n",
                "import nbdev"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# | export\n",
                "async def process_chunk(\n",
                "    url,\n",
                "    chunk,\n",
                "    chunk_number,\n",
                "    source,\n",
                "    async_supabase_client,\n",
                "    database_table_name,\n",
                "    export_folder,\n",
                "    is_replace_llm_metadata: bool = False,\n",
                "    debug_prn: bool = False,\n",
                "):\n",
                "    if debug_prn:\n",
                "        print(f\"üé¨ starting {url} - {chunk_number}\")\n",
                "\n",
                "    chunk_path = (\n",
                "        f\"{export_folder}/chunks/{amcv.convert_url_file_name(url)}/{chunk_number}.md\"\n",
                "    )\n",
                "\n",
                "    chunk = pc.Crawler_ProcessedChunk.from_chunk(\n",
                "        chunk=chunk,\n",
                "        chunk_number=chunk_number,\n",
                "        url=url,\n",
                "        source=source,\n",
                "        output_path=chunk_path,\n",
                "    )\n",
                "\n",
                "    # try:\n",
                "    await chunk.generate_metadata(\n",
                "        output_path=chunk_path,\n",
                "        is_replace_llm_metadata=is_replace_llm_metadata,\n",
                "        debug_prn=debug_prn,\n",
                "    )\n",
                "\n",
                "    data = chunk.to_json()\n",
                "    data.pop(\"source\")\n",
                "\n",
                "    await storage_routes.store_data_in_supabase_table(\n",
                "        async_supabase_client=async_supabase_client,\n",
                "        table_name=database_table_name,\n",
                "        data=data,\n",
                "    )\n",
                "\n",
                "    if debug_prn:\n",
                "        print(f\"successfully processed {url}-{chunk_number}\")\n",
                "\n",
                "    return chunk\n",
                "\n",
                "    # except Exception as e:\n",
                "    #     print(\n",
                "    #         utils.generate_error_message(\n",
                "    #             f\"üíÄ process_chunk - {url} - {chunk_number} -{e}\", exception=e\n",
                "    #         )\n",
                "    #     )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# | export\n",
                "async def read_url(\n",
                "    url,\n",
                "    source,\n",
                "    browser_config: crawler_routes.BrowserConfig,\n",
                "    doc_path,\n",
                "    crawler_config: crawler_routes.CrawlerRunConfig = None,\n",
                "    debug_prn: bool = False,\n",
                "):\n",
                "    if os.path.exists(doc_path):\n",
                "        content, _ = amfi.read_md_from_disk(doc_path)\n",
                "\n",
                "        if debug_prn:\n",
                "            print(f\"üõ¢Ô∏è  {url} - scraping not required, file retrieved from - {doc_path}\")\n",
                "\n",
                "        return content\n",
                "    \n",
                "    storage_fn = partial(storage_routes.save_chunk_to_disk,\n",
                "        output_path=doc_path,\n",
                "        )\n",
                "\n",
                "    res = await crawler_routes.scrape_url(\n",
                "        url=url,\n",
                "        session_id=source,\n",
                "        browser_config=browser_config,\n",
                "        crawler_config=crawler_config,\n",
                "        storage_fn = storage_fn\n",
                "    )\n",
                "    if debug_prn:\n",
                "        print(f\"üõ¢Ô∏è  {url} - page scraped to {doc_path}\")\n",
                "\n",
                "    return res.markdown"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# | export\n",
                "async def process_url(\n",
                "    url: str,\n",
                "    source: str,\n",
                "    export_folder: str,\n",
                "    database_table_name: str,\n",
                "    async_supabase_client=None,\n",
                "    debug_prn: bool = False,\n",
                "    browser_config: crawler_routes.BrowserConfig = None,\n",
                "    crawler_config: crawler_routes.CrawlerRunConfig = None,\n",
                "    is_replace_llm_metadata: bool = False,\n",
                "    max_conccurent_requests=5,\n",
                "):\n",
                "    \"\"\"process a document and store chunks in parallel\"\"\"\n",
                "\n",
                "    browser_config = browser_config or crawler_routes.default_browser_config\n",
                "    async_supabase_client = async_supabase_client or storage_routes.async_supabase_client\n",
                "\n",
                "    doc_path = f\"{export_folder}/{amcv.convert_url_file_name(url)}.md\"\n",
                "\n",
                "    ## scrape url and save results to doc_path\n",
                "    try:\n",
                "        if debug_prn:\n",
                "            print(f\"starting crawl - {url}\")\n",
                "\n",
                "        markdown = await read_url(\n",
                "            url=url,\n",
                "            source=source,\n",
                "            browser_config=browser_config,\n",
                "            doc_path=doc_path,\n",
                "            debug_prn=debug_prn,\n",
                "            crawler_config=crawler_config,\n",
                "        )\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"‚õî  {url} - error while read_url - {e}\")\n",
                "        return False\n",
                "\n",
                "    if debug_prn:\n",
                "        print(f\"‚òÄÔ∏è  successfully crawled: {url}\")\n",
                "\n",
                "    chunks = amcn.chunk_text(markdown)\n",
                "\n",
                "    if debug_prn:\n",
                "        print(f\"‚òÄÔ∏è  : {len(chunks)} to process {url}\")\n",
                "\n",
                "    res = await amce.gather_with_concurrency(\n",
                "        *[\n",
                "            process_chunk(\n",
                "                url=url,\n",
                "                chunk=chunk,\n",
                "                chunk_number=idx,\n",
                "                source=source,\n",
                "                async_supabase_client=async_supabase_client,\n",
                "                database_table_name=database_table_name,\n",
                "                export_folder=export_folder,\n",
                "                debug_prn=debug_prn,\n",
                "                is_replace_llm_metadata=is_replace_llm_metadata,\n",
                "            )\n",
                "            for idx, chunk in enumerate(chunks)\n",
                "        ],\n",
                "        n=max_conccurent_requests,\n",
                "    )\n",
                "\n",
                "    if debug_prn:\n",
                "        print(f\"‚òÄÔ∏è  done processing url {url}\")\n",
                "\n",
                "    return res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| export\n",
                "async def process_rgd(\n",
                "    rgd,\n",
                "    source : str, \n",
                "    export_folder: str,\n",
                "    database_table_name: str = \"site_pages\",\n",
                "    supabase_client=None,\n",
                "    debug_prn: bool = False,\n",
                "    is_replace_llm_metadata: bool = False,\n",
                "    max_conccurent_requests=5,\n",
                "):\n",
                "\n",
                "    supabase_client = supabase_client or storage_routes.async_supabase_client\n",
                "\n",
                "    ## scrape url and save results to doc_path\n",
                "    if debug_prn:\n",
                "        print(f\"processing - {rgd.url}\")\n",
                "\n",
                "    chunks = amcn.chunk_text(rgd.markdown)\n",
                "\n",
                "    if debug_prn:\n",
                "        print(f\"‚òÄÔ∏è  : {len(chunks)} to process {rgd.url}\")\n",
                "\n",
                "    res = await amce.gather_with_concurrency(\n",
                "        *[\n",
                "            process_chunk(\n",
                "                url=rgd.url,\n",
                "                chunk=chunk,\n",
                "                chunk_number=idx,\n",
                "                source=source,\n",
                "                async_supabase_client=supabase_client,\n",
                "                database_table_name=database_table_name,\n",
                "                export_folder=export_folder,\n",
                "                debug_prn=debug_prn,\n",
                "                is_replace_llm_metadata=is_replace_llm_metadata,\n",
                "            )\n",
                "            for idx, chunk in enumerate(chunks)\n",
                "        ],\n",
                "        n=max_conccurent_requests,\n",
                "    )\n",
                "\n",
                "    if debug_prn:\n",
                "        print(f\"‚òÄÔ∏è  done processing url {rgd.url}\")\n",
                "\n",
                "    return res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# | export\n",
                "async def process_urls(\n",
                "    urls: List[str | None],\n",
                "    source: str,\n",
                "    export_folder: str = \"./export\",\n",
                "    database_table_name: str = \"site_pages\",\n",
                "    max_conccurent_requests: int = 5,\n",
                "    debug_prn: bool = False,\n",
                "    browser_config : crawler_routes.BrowserConfig =None,\n",
                "    crawler_config : crawler_routes.CrawlerRunConfig = None,\n",
                "    is_replace_llm_metadata: bool = False,\n",
                "):\n",
                "    if not urls:\n",
                "        print(\"No URLs found to crawl\")\n",
                "        return\n",
                "\n",
                "    urls_path = f\"./export/urls/{source}.txt\"\n",
                "\n",
                "    amfi.upsert_folder(urls_path)\n",
                "\n",
                "    with open(urls_path, \"w+\", encoding=\"utf-8\") as f:\n",
                "        f.write(\"\\n\".join(urls))\n",
                "\n",
                "    res = await amce.gather_with_concurrency(\n",
                "        *[\n",
                "            process_url(\n",
                "                url=url,\n",
                "                source=source,\n",
                "                debug_prn=debug_prn,\n",
                "                browser_config=browser_config,\n",
                "                export_folder=export_folder,\n",
                "                database_table_name=database_table_name,\n",
                "                is_replace_llm_metadata=is_replace_llm_metadata,\n",
                "                crawler_config=crawler_config\n",
                "            )\n",
                "            for url in urls\n",
                "        ],\n",
                "        n=max_conccurent_requests,\n",
                "    )\n",
                "\n",
                "    print(\"done\")\n",
                "    return res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "#| hide\n",
                "nbdev.nbdev_export()"
            ]
        }
    ],
    "metadata": {
        "jupytext": {
            "split_at_heading": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
