{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# default_exp implementations.scrape_urls"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Default Title (change me)\n", "> Default description (change me)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "import os\n", "from typing import List\n", "\n", "import utils\n", "\n", "from src.routes import storage as storage_routes\n", "from src.routes import crawler as crawler_routes\n", "from src.classes import Crawler_ProcessedChunk as pc\n", "import utils.RagError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "async def process_chunk(\n", "    url,\n", "    chunk,\n", "    chunk_number,\n", "    source,\n", "    supabase_client,\n", "    database_table_name,\n", "    export_folder,\n", "    is_replace_llm_metadata: bool = False,\n", "    debug_prn: bool = False,\n", "):\n", "    if debug_prn:\n", "        print(f\"\ud83c\udfac starting {url} - {chunk_number}\")\n", "\n", "    chunk_path = (\n", "        f\"{export_folder}/chunks/{utils.convert_url_file_name(url)}/{chunk_number}.md\"\n", "    )\n", "\n", "    chunk = pc.Crawler_ProcessedChunk.from_chunk(\n", "        chunk=chunk,\n", "        chunk_number=chunk_number,\n", "        url=url,\n", "        source=source,\n", "        output_path=chunk_path,\n", "    )\n", "\n", "    # try:\n", "    await chunk.generate_metadata(\n", "        output_path=chunk_path,\n", "        is_replace_llm_metadata=is_replace_llm_metadata,\n", "        debug_prn=debug_prn,\n", "    )\n", "\n", "    data = chunk.to_json()\n", "    data.pop(\"source\")\n", "\n", "    storage_routes.store_data_in_supabase_table(\n", "        supabase_client=supabase_client,\n", "        table_name=database_table_name,\n", "        data=data,\n", "    )\n", "\n", "    if debug_prn:\n", "        print(f\"successfully processed {url}-{chunk_number}\")\n", "\n", "    return chunk\n", "\n", "    # except Exception as e:\n", "    #     print(\n", "    #         utils.generate_error_message(\n", "    #             f\"\ud83d\udc80 process_chunk - {url} - {chunk_number} -{e}\", exception=e\n", "    #         )\n", "    #     )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "async def read_url(url, source, browser_config, doc_path, debug_prn: bool = False):\n", "    if os.path.exists(doc_path):\n", "        content, _ = utils.read_md_from_disk(doc_path)\n", "\n", "        if debug_prn:\n", "            print(f\"\ud83d\udee2\ufe0f  {url} - scraping not required, file retrieved from - {doc_path}\")\n", "\n", "        return content\n", "\n", "    res = await crawler_routes.scrape_url(\n", "        url=url,\n", "        session_id=source,\n", "        browser_config=browser_config,\n", "        output_path=doc_path,\n", "    )\n", "    if debug_prn:\n", "        print(f\"\ud83d\udee2\ufe0f  {url} - page scraped to {doc_path}\")\n", "\n", "    return res.markdown"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "async def process_url(\n", "    url: str,\n", "    source: str,\n", "    export_folder: str,\n", "    database_table_name: str,\n", "    supabase_client=None,\n", "    debug_prn: bool = False,\n", "    browser_config=None,\n", "    is_replace_llm_metadata: bool = False,\n", "    max_conccurent_requests=5,\n", "):\n", "    \"\"\"process a document and store chunks in parallel\"\"\"\n", "\n", "    browser_config = browser_config or crawler_routes.default_browser_config\n", "    supabase_client = supabase_client or storage_routes.default_supabase_client\n", "\n", "    doc_path = f\"{export_folder}/{utils.convert_url_file_name(url)}.md\"\n", "\n", "    ## scrape url and save results to doc_path\n", "    try:\n", "        if debug_prn:\n", "            print(f\"starting crawl - {url}\")\n", "\n", "        markdown = await read_url(\n", "            url=url,\n", "            source=source,\n", "            browser_config=browser_config,\n", "            doc_path=doc_path,\n", "            debug_prn=debug_prn,\n", "        )\n", "\n", "    except Exception as e:\n", "        print(f\"\u26d4  {url} - error while read_url - {e}\")\n", "        return False\n", "\n", "    if debug_prn:\n", "        print(f\"\u2600\ufe0f  successfully crawled: {url}\")\n", "\n", "    chunks = utils.chunk_text(markdown)\n", "\n", "    if debug_prn:\n", "        print(f\"\u2600\ufe0f  : {len(chunks)} to process {url}\")\n", "\n", "    res = await utils.gather_with_concurrency(\n", "        *[\n", "            process_chunk(\n", "                url=url,\n", "                chunk=chunk,\n", "                chunk_number=idx,\n", "                source=source,\n", "                supabase_client=supabase_client,\n", "                database_table_name=database_table_name,\n", "                export_folder=export_folder,\n", "                debug_prn=debug_prn,\n", "                is_replace_llm_metadata=is_replace_llm_metadata,\n", "            )\n", "            for idx, chunk in enumerate(chunks)\n", "        ],\n", "        n=max_conccurent_requests,\n", "    )\n", "\n", "    if debug_prn:\n", "        print(f\"\u2600\ufe0f  done processing url {url}\")\n", "\n", "    return res"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "async def process_urls(\n", "    urls: List[str | None],\n", "    source: str,\n", "    export_folder: str = \"./export\",\n", "    database_table_name: str = \"site_pages\",\n", "    max_conccurent_requests: int = 5,\n", "    debug_prn: bool = False,\n", "    browser_config=None,\n", "    is_replace_llm_metadata: bool = False,\n", "):\n", "    if not urls:\n", "        print(\"No URLs found to crawl\")\n", "        return\n", "\n", "    urls_path = f\"./export/urls/{source}.txt\"\n", "\n", "    utils.upsert_folder(urls_path)\n", "\n", "    with open(urls_path, \"w+\", encoding=\"utf-8\") as f:\n", "        f.write(\"\\n\".join(urls))\n", "\n", "    res = await utils.gather_with_concurrency(\n", "        *[\n", "            process_url(\n", "                url=url,\n", "                source=source,\n", "                debug_prn=debug_prn,\n", "                browser_config=browser_config,\n", "                export_folder=export_folder,\n", "                database_table_name=database_table_name,\n", "                is_replace_llm_metadata=is_replace_llm_metadata,\n", "            )\n", "            for url in urls\n", "        ],\n", "        n=max_conccurent_requests,\n", "    )\n", "\n", "    print(\"done\")\n", "    return res"]}], "metadata": {"jupytext": {"split_at_heading": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 4}