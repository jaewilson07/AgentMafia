{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# default_exp implementations.streamlit_ui"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Default Title (change me)\n", "> Default description (change me)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "from __future__ import annotations\n", "from functools import partial\n", "\n", "from dotenv import load_dotenv\n", "\n", "from typing import Literal, TypedDict\n", "from enum import Enum\n", "import os\n", "\n", "import streamlit as st\n", "import logfire\n", "from supabase import Client as SupabaseClient\n", "from openai import AsyncOpenAI\n", "\n", "from pydantic_ai.messages import (\n", "    ModelResponse,\n", "    TextPart,\n", ")\n", "\n", "from src import PydanticAIDependencies, pydantic_ai_expert\n", "\n", "\n", "load_dotenv()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "supabase: SupabaseClient = SupabaseClient(\n", "    os.getenv(\"SUPABASE_URL\"), os.getenv(\"SUPABASE_SERVICE_KEY\")\n", ")\n", "\n", "# Configure logfire to suppress warnings (optional)\n", "logfire.configure(send_to_logfire=\"never\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "class ChatMessage(TypedDict):\n", "    \"\"\"Format of messages setn to the browser/API\"\"\"\n", "\n", "    role: Literal[\"user\", \"model\"]\n", "    timestamp: str\n", "    content: str"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "def display_system_prompt(content):\n", "    with st.chat_message(\"system\"):\n", "        st.markdown(f\"**System**: {content}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "def display_user_prompt(content):\n", "    with st.chat_message(\"user\"):\n", "        st.markdown(content)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "def display_text(content):\n", "    with st.chat_message(\"assistant\"):\n", "        st.markdown(content)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "def display_message_part(part):\n", "    \"\"\"\n", "    Display a single part of a message in the Streamlit UI.\n", "    Customize how you display system prompts, user prompts,\n", "    tool calls, tool returns, etc.\n", "    \"\"\"\n", "\n", "    class MessagePartEnum(Enum):\n", "        \"\"\"enum for parsing the correct function to use to display a part_kind\"\"\"\n", "\n", "        SYSTEM_PROMPT = partial(display_system_prompt)\n", "        USER_PROMPT = partial(display_user_prompt)\n", "        TEXT = partial(display_text)\n", "\n", "    MessagePartEnum[part.part_kind.replace(\"-\", \"_\").upper()].value(\n", "        content=part.content\n", "    )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "async def run_agent_with_streaming(user_input: str):\n", "    \"\"\"\n", "    Run the agent with streaming text for the user_input prompt,\n", "    while maintaining the entire conversation in `st.session_state.messages`.\n", "    \"\"\"\n", "\n", "    dependencies = PydanticAIDependencies(\n", "        supabase=supabase, openai_client=openai_client\n", "    )\n", "\n", "    async with pydantic_ai_expert.run_stream(\n", "        user_prompt=user_input,\n", "        deps=dependencies,\n", "        message_history=st.session_state.messages[\n", "            :-1\n", "        ],  # pass entire conversation so far\n", "    ) as result:\n", "        # gather partial text to show streaming results incrementally\n", "\n", "        partial_text = \"\"\n", "\n", "        message_placeholder = st.empty()\n", "\n", "        # render partial_text as it arrives\n", "        async for chunk in result.stream_text(dela=True):\n", "            partial_text += chunk\n", "            message_placeholder.markdown(partial_text)\n", "\n", "        # add new messages excluding initial user_prompt\n", "        filtered_messages = [\n", "            msg\n", "            for msg in result.new_messages()\n", "            if not (hasattr(msg, \"parts\") and any.part.part_kind == \"user-prompt\")\n", "        ]\n", "\n", "        st.session_state.messages.extend(filtered_messages)\n", "\n", "        # add final response to the messages\n", "        st.session_state.messages.append(\n", "            ModelResponse(parts=[TextPart(content=partial_text)])\n", "        )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#export\n", "async def pydantic_streamlit_ui():\n", "    st.title(\"Pydantic AI Agenti RAG\")\n", "    st.write(\"Ask me any question about Pydantic AI\")\n", "\n", "    # initialize chat history\n", "    # if \"messages\" not in st.session_state:\n", "    #     st.session_state.messages = []\n", "\n", "    # display all messages so far\n", "    # each message is either a ModelRequest or ModelResponse\n", "    # for msg in st.session_state.messages:\n", "    #     if isinstance(msg, ModelRequest) or isinstance(msg, ModelResponse):\n", "    #         for part in msg.parts:\n", "    #             display_message_part(part)\n", "\n", "    user_input = st.chat_input(\"What questions do you have about Pydantic AI?\")\n", "\n", "    # if user_input:\n", "    #     st.session_state.messages.append(\n", "    #         ModelRequest(parts=[UserPromptPart(content=user_input)])\n", "    #     )\n", "\n", "    # with st.chat_message(\"assistant\"):\n", "    #     await run_agent_with_streaming(user_input)"]}], "metadata": {"jupytext": {"split_at_heading": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 4}